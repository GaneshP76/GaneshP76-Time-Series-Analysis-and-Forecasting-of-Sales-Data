{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G7_TIME-SERIES ANALYSIS OF RETAIL SALES DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets and some necessary libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.graphics.tsaplots import acf, pacf, plot_acf, plot_pacf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('data/sales.csv')\n",
    "ext_variables_df = pd.read_csv('data/External Variables.csv')\n",
    "\n",
    "# Converting date column to datetime format\n",
    "ext_variables_df['Date'] = pd.to_datetime(ext_variables_df['Date'])\n",
    "sales_df['Date'] = pd.to_datetime(sales_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the size of our dataframes\n",
    "print(ext_variables_df.shape)\n",
    "print(sales_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the dataframe\n",
    "\n",
    "df = pd.merge(sales_df,ext_variables_df, on=['Store','Date', 'IsHoliday'], how='left')\n",
    "\n",
    "# describing the final dataframe that we have\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have some NaN values in some numeric columns such as MarkDown1 etc, so we will fill them with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace= True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities function for EDA  to avoid code redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes time series as an input and prints the result of Dickey-Fuller test as a result \n",
    "\n",
    "'''  initial idea is to use an AR model with linear regression since we have external factors to consider and \n",
    "since The Dickey Fuller test checks the null hypothesis if a unit root is present in an autoregressive mode we choose to \n",
    "test the stationarity using this test'''\n",
    "\n",
    "def adf_test(weekly_sales):\n",
    "    #Perform Dickey-Fuller test:\n",
    "    #print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(weekly_sales, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "           dfoutput['Critical Value (%s)'%key] = value\n",
    "    \n",
    "    #print (dfoutput)\n",
    "    return dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take store_id and data_frame as inputs and returns all weekly sales for specified store as time series\n",
    "def get_Weekly_sales_as_time_series_by_store_id(store_id, data_frame):\n",
    "    df_result=data_frame.where(data_frame['Store'] == store_id)\n",
    "    df_result=df_result.dropna()\n",
    "    df_result=df_result.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\n",
    "    df_result = df_result.set_index('Date')\n",
    "    df_result.sort_values('Date', inplace=True)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take store_id and data_frame as inputs and returns all external variables for specified store\n",
    "def get_external_variables_by_store_id(store_id, data_frame):\n",
    "    external_variables_df=data_frame.where(data_frame['Store'] == store_id)\n",
    "    external_variables_df=external_variables_df.dropna()\n",
    "    external_variables_df=external_variables_df.groupby(by=['Date'], as_index=False)[['Temperature', 'Fuel_Price', 'CPI', \\\n",
    "                                                        'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\\\n",
    "                                                                                      'MarkDown5']].mean()\n",
    "    external_variables_df = external_variables_df.set_index('Date')\n",
    "    external_variables_df.sort_values('Date', inplace=True)\n",
    "    return external_variables_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Functions to Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function plots sales of a fiven store against time, helps us to visualize the sales and to some extent if the\n",
    "### series is stationary\n",
    "\n",
    "def plot_sales(time_series):\n",
    "    \n",
    "    _ = plt.figure(figsize=(20,5))\n",
    "    _ = plt.plot(time_series.index, time_series.values, label = 'Time Series')\n",
    "    _ = rolling_mean = time_series.rolling(8).mean()\n",
    "    _ = rolling_mean = rolling_mean.dropna()\n",
    "    _ = plt.plot(rolling_mean, color='red', label='Rolling Mean')\n",
    "    _ = plt.ylabel(\"Sales ($)\", fontsize = 15)\n",
    "    _ = plt.xlabel(\"Date\", fontsize = 15)\n",
    "    _ = plt.legend(fontsize = 15)\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function plots the pacf and acf curves for the store which will \n",
    "### help us in identifying AR parameters for our time series model\n",
    "\n",
    "def plot_acf_and_pacf(time_series):\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2, figsize=(20,5))\n",
    "    plot_acf(time_series.values,lags=70, ax=axes[0])\n",
    "    plot_pacf(time_series.values,lags=70, ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of external variables on Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the effect of external variables on weekly sales of a store, store 42 for example. We will use the functions we wrote to plot the sales curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of External Variables with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_store_42 = get_external_variables_by_store_id(42, df) \n",
    "store_42_ts = get_Weekly_sales_as_time_series_by_store_id(42, df) \n",
    "ext_store_42['Date'] = ext_store_42.index\n",
    "ext_store_42['Weekly_Sales'] = store_42_ts['Weekly_Sales']\n",
    "ext_store_42[['Date', 'Unemployment', 'Fuel_Price', 'Temperature', 'CPI', 'MarkDown1', \\\n",
    "    'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].plot(x='Date', subplots=True, figsize=(20,30))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity Test for Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our EDA we check the stationarity of stores through ADF function we have written earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stores = list(set(df['Store']))\n",
    "stores_dicky_fuller = []\n",
    "stationary_stores = []\n",
    "\n",
    "for store in all_stores:\n",
    "    store_sales = get_Weekly_sales_as_time_series_by_store_id(store, df)\n",
    "    adf_test_result = adf_test(store_sales['Weekly_Sales'])\n",
    "    (store, pvalue, test_statistic) = store,adf_test_result['p-value'],adf_test_result['Test Statistic']\n",
    "    stores_dicky_fuller.append((store, pvalue, test_statistic))\n",
    "    if (pvalue < 0.05) and (adf_test_result['Test Statistic'] < adf_test_result['Critical Value (1%)']):\n",
    "        stationary_stores.append(store)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stationary Stores:', stationary_stores)\n",
    "non_stationary_stores = [store_num for store_num in all_stores if store_num not in stationary_stores]\n",
    "print('')\n",
    "print('Non Stationary Stores:', non_stationary_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list for stationary and non-stationary stores, we visualize the sales figures for some of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by picking store '42' for stationary and store '30' for non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_42_ts = get_Weekly_sales_as_time_series_by_store_id(42, df)\n",
    "store_42_external_variables = get_external_variables_by_store_id(42, df)\n",
    "plot_sales(store_42_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_30_ts = get_Weekly_sales_as_time_series_by_store_id(30, df)\n",
    "store_30_external_variables = get_external_variables_by_store_id(30, df)\n",
    "plot_sales(store_30_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visualization we see that both have sort of a varying rolling mean but the fluctuation is more apparent in store_30 and hence adf test validates non-stationarity of the store. As we see from these time series plots it is very diffcult to just go by a mere visual appeal to test stationarity and hence the adf test helps. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the ADF test results for both the stores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = adf_test(store_42_ts[\"Weekly_Sales\"])\n",
    "print(c)\n",
    "c['p-value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = adf_test(store_30_ts[\"Weekly_Sales\"])\n",
    "print(c)\n",
    "c['p-value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 'p-value' for store_42 is less that 0.05 while for store_30 it is 0.025. Looking at the result above p-Value is < 0.05 and Test Statistic is < Critical Value for store_42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use Box-Jenkins methodology to work on and assess our model.\n",
    "* As we are forcasting the sales of different stores (by summing up the sales of all of their departments) and we have time series, and after examing our data we decided to use AR model. In order to consider external variables we will need to use Linear Regression.Therefore, we worte these custom functions below (fit_model_ar_with_external_variables(), predict_model_ar_with_external_variables()) to consider both Autoregression and Linear regression\n",
    "\n",
    "* As the datasets we used are from 2010 - 2012, we will train our model on 2010-2011 and do the prediction on 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression Time Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Model Fit and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_ar_with_external_variables(time_series, order_array, external_variables):\n",
    "\n",
    "    data_x = []\n",
    "    for index in range(np.max(order_array), len(time_series)):\n",
    "            \n",
    "            data_x.append(time_series.values[(index-order_array)].squeeze())\n",
    "    \n",
    "    data_x = np.array(data_x)\n",
    "    data_x = np.append(data_x, external_variables.values[np.max(order_array):], axis=1)\n",
    "    data_y = time_series[-len(data_x):].values\n",
    "    lr_model=LinearRegression()\n",
    "    lr_model.fit(data_x,data_y.ravel())\n",
    "    print('Model Score: %.2f' % lr_model.score(data_x,data_y))\n",
    "    print(\"coefficients:\", lr_model.coef_)\n",
    "    return lr_model.coef_, lr_model.intercept_\n",
    "\n",
    "def predict_model_ar_with_external_variables(time_series, order_array, external_variables, coef, intercept):\n",
    "    \n",
    "    data_x = []\n",
    "    for index in range(np.max(order_array), len(time_series)):\n",
    "            \n",
    "        data_x.append(time_series.values[(index-order_array)].squeeze())\n",
    "    \n",
    "    data_x = np.array(data_x)\n",
    "    data_x = np.append(data_x, external_variables.values[np.max(order_array):], axis=1)\n",
    "    data_x = np.array(np.dot(data_x, coef.T) + intercept)\n",
    "    return data_x\n",
    "\n",
    "def fit_model_ar(time_series, order_array):\n",
    "\n",
    "    data_x = []\n",
    "    for index in range(np.max(order_array), len(time_series)):\n",
    "    \n",
    "        data_x.append(time_series.values[(index-order_array)].squeeze())\n",
    "\n",
    "    data_x = np.array(data_x)\n",
    "    data_y = time_series[-len(data_x):].values\n",
    "    lr_model=LinearRegression()\n",
    "    lr_model.fit(data_x,data_y)\n",
    "    print('Model Score: %.2f' % lr_model.score(data_x,data_y))\n",
    "    print(\"coefficients:\", lr_model.coef_)\n",
    "    return lr_model.coef_, lr_model.intercept_\n",
    "\n",
    "def predict_model_ar(time_series, order_array, coef, intercept):\n",
    "    \n",
    "    data = []\n",
    "    for index in range(np.max(order_array), len(time_series)):\n",
    "        \n",
    "        data.append(np.sum(np.dot(coef, time_series.values[(index-order_array)].squeeze())) + intercept)\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plot for actual and predicted values.\n",
    "\n",
    "def plot_predicted_and_actual_sales(actual, prediction):\n",
    "\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(actual, label='Actual Sales')\n",
    "    plt.plot(prediction, label='Predicted Sales')\n",
    "    _ = plt.ylabel(\"Sales ($)\", fontsize = 15)\n",
    "    _ = plt.xlabel(\"Date\", fontsize = 15)\n",
    "    _ = plt.legend(fontsize = 15)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_plot_AR_Residuals(actual, prediction):\n",
    "    \n",
    "    difference=(actual['Weekly_Sales']-prediction[0])/actual['Weekly_Sales']\n",
    "    print('Residuals: Mean %.2f, std %.2f' % (difference.mean(), difference.std()))\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(difference, c='green')\n",
    "    _ = plt.ylabel(\"Sales ($)\", fontsize = 15)\n",
    "    _ = plt.xlabel(\"Date\", fontsize = 15)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff_of_last_five_weeks(actual, prediticed):\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"Actual           \", \"##############\", \"Predicted           \")\n",
    "        print(float(actual[\"Weekly_Sales\"][(len(actual) - 1) - i]),\"##############\", float(prediticed[0][(len(prediticed) - 1) - i]))\n",
    "    print()\n",
    "    for i in range(5):\n",
    "        print(\"Difference        \", \"##############\", \"Precentage           \")\n",
    "        act = float(actual[\"Weekly_Sales\"][(len(actual) - 1) - i])\n",
    "        pred_num = float(prediticed[0][(len(prediticed) - 1) - i])\n",
    "        diff = act - pred_num\n",
    "        percent = (diff * 100)/act\n",
    "        print(diff, \" ##############\", percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling and Analysis for Stationary Data (Store_42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the EDA we saw that the store_42 passed the stationary test. Hence we start off by modelling our first time series model using this store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start off by plotting ACF and PACF for the store by using the functions written earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_42_ts = get_Weekly_sales_as_time_series_by_store_id(42, df)\n",
    "store_42_external_variables = get_external_variables_by_store_id(42, df)\n",
    "plot_sales(store_42_ts)\n",
    "plot_acf_and_pacf(store_42_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* worth mentioning: our data has 143 weeks out of which 43 are related to 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(store_42_ts))\n",
    "display(store_42_ts.iloc[:100])\n",
    "display(store_42_ts.iloc[100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* looking at the tables above we will notice that 2012 starts from week number 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will do fitting and predictiong (without external variables) based on the result above. In PACF result we noticed that our significant values are\n",
    "1,3,46,60,69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got these numbers from our observation above, but we will add 100 at the end as our prediciton function will start from the\n",
    "# last week (biggest number) which will be 100 in this case\n",
    "significants=np.array([1,3,46,60,69, 100])\n",
    "coef, intercept = fit_model_ar(store_42_ts,significants)\n",
    "pred=pd.DataFrame(index=store_42_ts[100:].index, data=predict_model_ar(store_42_ts, significants, coef, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- our score factor is 0.27. The perfect score factore is 1.0. Now we will plot a comparision between actual and predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2012 starts from week 100\n",
    "plot_predicted_and_actual_sales(store_42_ts[100:], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will now apply the same steps (using external variables) and see if there is a significant difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_variables_df = get_external_variables_by_store_id(42, df)\n",
    "coef, intercept = fit_model_ar_with_external_variables(store_42_ts,significants, external_variables_df)\n",
    "pred_with_ext=pd.DataFrame(index=store_42_ts[100:].index, data=predict_model_ar_with_external_variables(store_42_ts, significants,external_variables_df, coef, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- looking at the difference in score factore (0.27 without ext variables) and (0.56 with ext variables). The difference is big; therefore, considering external variables is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision between actual and predicted data using both with and without ext variables for store_42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(store_42_ts[100:], label='Actual Sales')\n",
    "plt.plot(pred, label='Predicted Sales (without ext variabales)')\n",
    "plt.plot(pred_with_ext, label='Predicted Sales (with ext variabales)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As shown above considering external variables is way more accurate. Therefore, we will consider fitting and modeling the next  stores without testing the prediction using no external variables as what we have done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the AR model for store_42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will now calculate the AR residual of store number 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot_AR_Residuals(store_42_ts[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* printing the difference of the last five weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_diff_of_last_five_weeks(store_42_ts[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE Error for Store_42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_store_42 = np.sqrt(mean_squared_error(store_42_ts[100:],pred_with_ext))\n",
    "print('RMSE Error on Test data for Store 42:',rms_store_42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will repeat the last 2 phases (Modeling and results) and (Evaluation) for store number 17, just to test our model on another stationary store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling and Analysis for Stationary Data (Store_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_17_ts = get_Weekly_sales_as_time_series_by_store_id(17, df)\n",
    "store_17_external_variables = get_external_variables_by_store_id(17, df)\n",
    "plot_sales(store_17_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* applying Dicky-Fuller test as a further check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(store_17_ts[\"Weekly_Sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* According to Dicky-Fuller test the data is stationary, except the fact that we have 2 trends that we're going to solve by consedring significants in PACF chart and then using our 2 custome fitting and predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_and_pacf(store_17_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will do fitting and prediction based on the result above. In PACF result we noticed that our significant values are\n",
    "2,4,56,59,69,70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significants=np.array([2,4,45,56,69,70,100])\n",
    "external_variables_df = get_external_variables_by_store_id(17, df)\n",
    "coef, intercept = fit_model_ar_with_external_variables(store_17_ts,significants, external_variables_df)\n",
    "pred_with_ext=pd.DataFrame(index=store_17_ts[100:].index, data=predict_model_ar_with_external_variables(store_17_ts, significants,external_variables_df, coef, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have included the value 100 in our significants too since our prediciton function will start from this week and we have observed on trial and error basis that when including the last week the model gives better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- our score factor is 0.6. The perfect score factore is 1.0. Now we will plot a comparision between actual and predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012 starts from week 100\n",
    "plot_predicted_and_actual_sales(store_17_ts[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision between actual and predicted data using both with and without ext variables for store_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will now calculate the AR residual of store number 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot_AR_Residuals(store_17_ts[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* printing the difference of the last five weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE Error for Store_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_store_17 = np.sqrt(mean_squared_error(store_17_ts[100:],pred_with_ext))\n",
    "print('RMSE Error on test data for store 17:',rms_store_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_diff_of_last_five_weeks(store_17_ts[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Non-Stationary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling and Analysis for Non-Stationary Data (Store_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the EDA done earlier we have seen that store_30 was non-stationary as per the adf test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_30_ts = get_Weekly_sales_as_time_series_by_store_id(30, df)\n",
    "external_variables_df = get_external_variables_by_store_id(30, df)\n",
    "plot_sales(store_30_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(store_30_ts[\"Weekly_Sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after 3 lags the data is close to stationary as per the ADF test. We will try to take single dif transformation and check for the stationarity property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_30_diff = store_30_ts.diff().fillna(0)\n",
    "plot_sales(store_30_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(store_30_diff[\"Weekly_Sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now close to stationary from the plot we see that rolling mean is almost a flat curve. We will plot pacf and acf for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_and_pacf(store_30_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significants=np.array([1,2,3,4,59,60,61,62,100])\n",
    "coef, intercept = fit_model_ar_with_external_variables(store_30_diff,significants,external_variables_df)\n",
    "pred_with_ext=pd.DataFrame(index=store_30_diff[100:].index, data=predict_model_ar_with_external_variables(store_30_diff, significants,external_variables_df, coef, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision between actual and predicted data using both with and without ext variables for store_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predicted_and_actual_sales(store_30_diff[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot_AR_Residuals(store_30_diff[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model has got a good score and fit well after we have transformed the data to a single diffed series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_diff_of_last_five_weeks(store_30_diff[100:], pred_with_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE for Store_30_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_store_30 = np.sqrt(mean_squared_error(store_30_diff[100:],pred_with_ext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE Error on Test data for diffed Store 30:',rms_store_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach using Classical Time-Series Model ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea with this part of the implementation is to forecast on sales values without using the external variables(keeping it uni-variate) and considering integrated moving average value and AR values. \n",
    "\n",
    "We first approach this problem using AUTO ARIMA built-in function for grid search to find the best p,d,q parameters and then use them on the statsmodels ARIMA function and predict sales values on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### First we Define the Functions to model our data and visualize the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "def ARIMA_model(time_series,arima_order):\n",
    "    '''This functions takes the time series input, makes predictions on walk through validation basis and returns the predic\n",
    "    tions and updated training set,'''\n",
    "    X = time_series.values\n",
    "    train, test = X[0:100], X[100:]\n",
    "    train_updated = [x for x in train]\n",
    "    predictions = list()\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        model = ARIMA(train_updated, order=arima_order)\n",
    "        ARIMA_results = model.fit(trend='nc', disp=False)\n",
    "        yhat = ARIMA_results.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        act = test[i]\n",
    "        train_updated.append(act)\n",
    "    return (predictions,train_updated)\n",
    "\n",
    "\n",
    "def ARIMA_Visualize(time_series, predicted, trained):\n",
    "    '''This function takes the input of original time series, the predicted and the updated trained values\n",
    "    and returns plots to compare the actual test series with the  predicted values'''\n",
    "    predicted_series= pd.Series(predicted, index=time_series.index[100:])\n",
    "    trained_series= pd.Series(trained, index=time_series.index[:len(time_series)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(time_series[100:], label='Test')\n",
    "    #plt.plot(trained_series, label='Walk Throgh Validated')\n",
    "    plt.plot(predicted_series, label='predicted')\n",
    "    plt.legend(loc='upper left', fontsize=8)\n",
    "    plt.title('Forecast vs Actuals')\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA for store_42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Auto-regression we use the same stores to check for ARIMA and then evaluate using RMSE values.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search to find the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "store_42_ts = get_Weekly_sales_as_time_series_by_store_id(42,df)\n",
    "stepwise_fit = pm.auto_arima(store_42_ts, start_p=1, start_q=1, max_p=5,max_q=5,m=6,\n",
    "                             start_P=0, seasonal=False, d=1, D=1, trace=True,\n",
    "                             error_action='ignore',  # don't want to know if an order does not work\n",
    "                             suppress_warnings=True,  # don't want convergence warnings\n",
    "                             stepwise=True)  # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run the model and apply the visualization\n",
    "predicted_values = ARIMA_model(store_42_ts,(0,1,2))[0]\n",
    "ARIMA_Visualize(store_42_ts, predicted_values, ARIMA_model(store_42_ts,(0,1,2))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE for store_42 with ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_store_42_ARIMA = np.sqrt(mean_squared_error(store_42_ts[100:],predicted_values))\n",
    "print('RMSE Error on Test data for Store 42:',rmse_store_42_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA for store_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search to find the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "store_17_ts = get_Weekly_sales_as_time_series_by_store_id(17,df)\n",
    "stepwise_fit = pm.auto_arima(store_17_ts, start_p=1, start_q=1, max_p=5,max_q=5,m=6,\n",
    "                             start_P=0, seasonal=False, d=1, D=1, trace=True,\n",
    "                             error_action='ignore',  # don't want to know if an order does not work\n",
    "                             suppress_warnings=True,  # don't want convergence warnings\n",
    "                             stepwise=True)  # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run the model and apply the visualization\n",
    "predicted_values = ARIMA_model(store_17_ts,(1,1,2))[0]\n",
    "ARIMA_Visualize(store_17_ts, predicted_values, ARIMA_model(store_17_ts,(1,1,2))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE for store_17\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_store_17 = np.sqrt(mean_squared_error(store_17_ts[100:],predicted_values))\n",
    "print('RMSE Error on Test data for diffed Store 17:',rmse_store_17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA for Store_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "store_30_ts = get_Weekly_sales_as_time_series_by_store_id(30,df)\n",
    "stepwise_fit = pm.auto_arima(store_30_ts, start_p=1, start_q=1, max_p=5,max_q=5,m=6,\n",
    "                             start_P=0, seasonal=False, d=1, D=1, trace=True,\n",
    "                             error_action='ignore',  # don't want to know if an order does not work\n",
    "                             suppress_warnings=True,  # don't want convergence warnings\n",
    "                             stepwise=True)  # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run the model and apply the visualization\n",
    "predicted_values = ARIMA_model(store_30_ts,(1,1,0))[0]\n",
    "ARIMA_Visualize(store_30_ts, predicted_values, ARIMA_model(store_30_ts,(1,1,0))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_store_30 = np.sqrt(mean_squared_error(store_30_ts[100:],predicted_values))\n",
    "print('RMSE Error on Test data for Store 30:',rmse_store_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
